{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import loss\n",
    "import inlp\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.sample.25k.bert-large.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    \"\"\"Simple torch dataset class\"\"\"\n",
    "    def __init__(self, data, device):\n",
    "\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        with torch.no_grad():\n",
    "             \n",
    "            vec1_np, vec2_np, str1, str2, _ = self.data[index]\n",
    "            \n",
    "            vec1, vec2, str1, str2, pair_id = self.data[index]            \n",
    "            vec1, vec2 = torch.from_numpy(vec1_np).float(), torch.from_numpy(vec2_np).float()\n",
    "            \n",
    "            vec1 = vec1.to(self.device)\n",
    "            vec2 = vec2.to(self.device)\n",
    "            \n",
    "            return (vec1, vec2, str1, str2, pair_id)\n",
    "        \n",
    "        \n",
    "def get_nullspace_projection(W: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param W: the matrix over its nullspace to project\n",
    "    :return: the projection matrix\n",
    "    \"\"\"\n",
    "    nullspace_basis = scipy.linalg.null_space(W)  # orthogonal basis\n",
    "\n",
    "    nullspace_basis = nullspace_basis * np.sign(nullspace_basis[0][0])  # handle sign ambiguity\n",
    "    projection_matrix = nullspace_basis.dot(nullspace_basis.T)\n",
    "\n",
    "    return projection_matrix\n",
    "    \n",
    "    \n",
    "def get_rowspace_projection(W: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param W: the matrix over its nullspace to project\n",
    "    :return: the projection matrix\n",
    "    \"\"\"\n",
    "\n",
    "    w_basis = scipy.linalg.orth(W.T) # orthogonal basis\n",
    "    w_basis * np.sign(w_basis[0][0]) # handle sign ambiguity\n",
    "    P_W = w_basis.dot(w_basis.T) # orthogonal projection on W's rowspace\n",
    "    \n",
    "    return P_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, X_train, X_dev, dim, batch_size, dropout_rate, device):\n",
    "        super(Siamese, self).__init__()\n",
    "        self.l = torch.nn.Linear(1024, dim)\n",
    "        \n",
    "        self.train_data = Dataset(X_train, device)\n",
    "        self.dev_data = Dataset(X_dev, device)\n",
    "        self.train_gen = torch.utils.data.DataLoader(self.train_data, batch_size = batch_size, drop_last = False, shuffle=True)\n",
    "        self.dev_gen = torch.utils.data.DataLoader(self.dev_data, batch_size = batch_size, drop_last = False, shuffle=True)\n",
    "        self.loss_fn = loss.BatchHardTripletLoss(final = \"softmax\", device = device)\n",
    "        self.dropout = torch.nn.Dropout(p = dropout_rate)\n",
    "        self.acc = None\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), weight_decay = 1e-6)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "          h1 = self.l(x1)\n",
    "          h2 = self.l(x2)\n",
    "         \n",
    "          return h1, h2\n",
    " \n",
    "    def train_network(self, num_epochs):\n",
    "    \n",
    "      trainer = Trainer(max_nb_epochs = num_epochs, min_nb_epochs = num_epochs, show_progress_bar = True)\n",
    "      trainer.fit(self)\n",
    "\n",
    "      return self.acc   \n",
    "      \n",
    "    def get_weights(self):\n",
    "    \n",
    "        return self.l.weight.data.numpy()\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRED\n",
    "        x1, x2, str1, str2, ids = batch\n",
    "\n",
    "        h1, h2 = self.forward(self.dropout(x1), self.dropout(x2))\n",
    "        loss_val =  self.loss_fn(h1, h2, str1, str2, ids, index=0, evaluation = False)\n",
    "        \n",
    "        return {'loss': loss_val[0]}\n",
    "        \n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "    \n",
    "        # OPTIONAL\n",
    "        x1, x2, str1, str2, ids = batch\n",
    "        h1, h2 = self.forward(x1, x2)\n",
    "        loss_val =  self.loss_fn(h1, h2, str1, str2, ids, index=batch_nb, evaluation = True)\n",
    "        return {'val_loss': loss_val[0]}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        # OPTIONAL    \n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        print(\"Loss is {}\".format(avg_loss))\n",
    "        return {'avg_val_loss': avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        return torch.optim.Adam(self.parameters(), weight_decay = 1e-4)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return self.train_gen\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        # can also return a list of val dataloaders\n",
    "        return self.dev_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:      Name                  Type Params\n",
      "0        l                Linear  262 K\n",
      "1  loss_fn  BatchHardTripletLoss    0  \n",
      "2  dropout               Dropout    0  \n",
      "Validation sanity check:  80%|████████  | 4/5 [00:00<00:00,  5.10batch/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                            \n",
      "\u001b[A                                  \n",
      "\n",
      "Epoch 1:   0%|          | 0/25 [00:00<?, ?batch/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.3180398643016815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|████████  | 20/25 [00:03<00:00,  6.00batch/s, batch_nb=19, loss=0.276, v_nb=41]\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 21/25 [00:03<00:00,  6.75batch/s, batch_nb=19, loss=0.276, v_nb=41]\n",
      "Epoch 1:  88%|████████▊ | 22/25 [00:03<00:00,  6.57batch/s, batch_nb=19, loss=0.276, v_nb=41]\n",
      "Epoch 1:  92%|█████████▏| 23/25 [00:03<00:00,  6.44batch/s, batch_nb=19, loss=0.276, v_nb=41]\n",
      "Epoch 1:  96%|█████████▌| 24/25 [00:04<00:00,  6.37batch/s, batch_nb=19, loss=0.276, v_nb=41]\n",
      "Epoch 1: 100%|██████████| 25/25 [00:04<00:00,  7.00batch/s, batch_nb=19, loss=0.276, v_nb=41]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "                                                            \u001b[A\n",
      "\u001b[A\n",
      "Epoch 1: 100%|██████████| 25/25 [00:04<00:00,  6.04batch/s, batch_nb=19, loss=0.276, v_nb=41]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.21725492179393768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "net = Siamese(data[:20000], data[20000:], batch_size = 1024, dim = 256,dropout_rate = 0.1, device = device).to(device)\n",
    "net.train_network(num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00582838e-06 -5.45755029e-07  1.57393515e-07 ... -1.42492354e-07\n",
      "   3.40864062e-07 -1.39698386e-08]\n",
      " [-5.45755029e-07 -3.72529030e-08  7.79982656e-07 ...  3.34344804e-07\n",
      "   7.39237294e-08 -9.26665962e-08]\n",
      " [ 1.57393515e-07  7.79982656e-07  2.08616257e-07 ...  3.58559191e-08\n",
      "  -4.84287739e-08  2.05589458e-07]\n",
      " ...\n",
      " [-1.42492354e-07  3.34344804e-07  3.58559191e-08 ... -8.94069672e-08\n",
      "   1.87195837e-07 -6.42612576e-08]\n",
      " [ 3.40864062e-07  7.39237294e-08 -4.84287739e-08 ...  1.87195837e-07\n",
      "   1.78813934e-07  8.42846930e-08]\n",
      " [-1.39698386e-08 -9.26665962e-08  2.05589458e-07 ... -6.42612576e-08\n",
      "   8.42846930e-08 -1.78813934e-07]]\n"
     ]
    }
   ],
   "source": [
    "W = net.l.weight.detach().cpu().numpy()\n",
    "P_Rw = get_rowspace_projection(W)\n",
    "I = np.eye(P_Rw.shape[0])\n",
    "P_Nw = I - P_Rw\n",
    "P_Nw2 = get_nullspace_projection(W)\n",
    "vecs = np.array([d[0] for d in data[20000:]])\n",
    "strings = [d[2] for d in data[20000:]]\n",
    "vecs_transformed = W.dot(vecs.T).T\n",
    "vecs_transformed_nullspace = P_Nw2.dot(vecs.T).T\n",
    "print(P_Nw2-P_Nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 1024) (1024, 900)\n"
     ]
    }
   ],
   "source": [
    "W_r_basis = scipy.linalg.orth(W.T)\n",
    "print(W.shape, W_r_basis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_string(np_array):\n",
    "        return \"\\t\".join([\"%0.4f\" % float(x) for x in np_array])\n",
    "    \n",
    "with open(\"vecs.transformed.nullspace.tsv\", \"w\") as f:\n",
    "    for v in vecs_transformed_nullspace:\n",
    "        f.write(to_string(v) + \"\\n\")\n",
    "\n",
    "with open(\"labels.transformed.nullspace.tsv\", \"w\") as f:\n",
    "    for s in strings:\n",
    "        f.write(s + \"\\n\")\n",
    "        \n",
    "with open(\"vecs.transformed.tsv\", \"w\") as f:\n",
    "    for v in vecs_transformed:\n",
    "        f.write(to_string(v) + \"\\n\")\n",
    "\n",
    "with open(\"labels.transformed.tsv\", \"w\") as f:\n",
    "    for s in strings:\n",
    "        f.write(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1023\n",
      "124\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "np.linalg.matrix_rank(W)\n",
    "print(np.linalg.matrix_rank(vecs))\n",
    "print(np.linalg.matrix_rank(vecs_transformed_nullspace))\n",
    "print(np.linalg.matrix_rank(P_Rw.dot(vecs.T).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[AINFO:root:      Name                  Type Params\n",
      "0        l                Linear  102 K\n",
      "1  loss_fn  BatchHardTripletLoss    0  \n",
      "2  dropout               Dropout    0  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Loss is 0.28423216938972473\n",
      "Loss is 0.22954709827899933\n",
      "Loss is 0.20683249831199646\n",
      "Loss is 0.18630759418010712\n",
      "Loss is 0.17049802839756012\n",
      "Loss is 0.16003009676933289\n",
      "Loss is 0.15152859687805176\n",
      "Loss is 0.14518693089485168\n",
      "Loss is 0.14100699126720428\n",
      "Loss is 0.13558314740657806\n",
      "Loss is 0.13317079842090607\n",
      "Loss is 0.12940062582492828\n",
      "Loss is 0.12703098356723785\n",
      "Loss is 0.12439966201782227\n",
      "Loss is 0.12251582741737366\n",
      "Loss is 0.12137825042009354\n",
      "Loss is 0.11912421882152557\n",
      "Loss is 0.11834836006164551\n",
      "Loss is 0.1167420968413353\n",
      "Loss is 0.1148914247751236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "iteration: 0, accuracy: None:   0%|          | 0/6 [02:06<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "iteration: 0, accuracy: None:  17%|█▋        | 1/6 [02:06<10:32, 126.59s/it]\u001b[A\u001b[A\u001b[A\u001b[AINFO:root:      Name                  Type Params\n",
      "0        l                Linear  102 K\n",
      "1  loss_fn  BatchHardTripletLoss    0  \n",
      "2  dropout               Dropout    0  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.113502137362957\n",
      "======================================\n",
      "Loss is 0.2785147726535797\n",
      "Loss is 0.22862054407596588\n",
      "Loss is 0.20555132627487183\n",
      "Loss is 0.1855544000864029\n",
      "Loss is 0.1697065681219101\n",
      "Loss is 0.1592424511909485\n",
      "Loss is 0.15148137509822845\n",
      "Loss is 0.1455279439687729\n",
      "Loss is 0.1396777629852295\n",
      "Loss is 0.13603438436985016\n",
      "Loss is 0.1318574994802475\n",
      "Loss is 0.12972413003444672\n",
      "Loss is 0.127724289894104\n",
      "Loss is 0.12453426420688629\n",
      "Loss is 0.12258563935756683\n",
      "Loss is 0.12045826017856598\n",
      "Loss is 0.11883234977722168\n",
      "Loss is 0.11782550811767578\n",
      "Loss is 0.1165626123547554\n",
      "Loss is 0.11476627737283707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "iteration: 1, accuracy: None:  17%|█▋        | 1/6 [04:16<10:32, 126.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "iteration: 1, accuracy: None:  33%|███▎      | 2/6 [04:16<08:30, 127.65s/it]\u001b[A\u001b[A\u001b[A\u001b[AINFO:root:      Name                  Type Params\n",
      "0        l                Linear  102 K\n",
      "1  loss_fn  BatchHardTripletLoss    0  \n",
      "2  dropout               Dropout    0  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.11292089521884918\n",
      "======================================\n",
      "Loss is 0.27768227458000183\n",
      "Loss is 0.22878669202327728\n",
      "Loss is 0.20566412806510925\n",
      "Loss is 0.1848350316286087\n",
      "Loss is 0.17024558782577515\n",
      "Loss is 0.15844789147377014\n",
      "Loss is 0.15101775527000427\n",
      "Loss is 0.14527741074562073\n",
      "Loss is 0.13967938721179962\n",
      "Loss is 0.13666410744190216\n",
      "Loss is 0.13273440301418304\n",
      "Loss is 0.12939713895320892\n",
      "Loss is 0.12783721089363098\n"
     ]
    }
   ],
   "source": [
    "P, rowspace_projections, Ws = inlp.get_debiasing_projection(6, 1024, is_autoregressive = False, X_train = data, X_dev = data, dropout_rate = 0.05, device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, _, _ = P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_transformed_nullspace = P.dot(vecs.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vecs.transformed.nullspace.tsv\", \"w\") as f:\n",
    "    for v in vecs_transformed_nullspace:\n",
    "        f.write(to_string(v) + \"\\n\")\n",
    "\n",
    "with open(\"labels.transformed.nullspace.tsv\", \"w\") as f:\n",
    "    for s in strings:\n",
    "        f.write(s + \"\\n\")\n",
    "        \n",
    "with open(\"vecs.transformed.tsv\", \"w\") as f:\n",
    "    for v in vecs_transformed:\n",
    "        f.write(to_string(v) + \"\\n\")\n",
    "\n",
    "with open(\"labels.transformed.tsv\", \"w\") as f:\n",
    "    for s in strings:\n",
    "        f.write(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
